---
title: "Cluster Analysis for Crime Data"
author: "Shweta Bhat."
date: "16 July 2017"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: yes
---

**NOTE** Before starting this assignment please remember to clear your environment, you can do that by running the following code chunk

```{r}
rm(list = ls(all=TRUE))
```

# Agenda

* Get the data

* Data pre-processing

* Explore the data

* Hierarchical Clustering

* Kmeans Clustering

* Visualising Clusters and Evaluation


# Problem Description

* In the following Unsupervised Learning activity, you will perform cluster analysis on a dataset that has arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states

* The variable names in the dataset are self explanatory

* So, you will cluster US states based on the crime rates, which can then be passed on to public policy experts to gain insight from it


# Reading & Understanding the Data

* Read in the dataset

```{r}
# Use the setwd() function to get to the directory where the data is present
setwd("/Users/shwetabhat/Desktop/DataScience/ClusterAssignment")
crimeData=read.csv("/Users/shwetabhat/Desktop/DataScience/ClusterAssignment/crime_data.csv",header=TRUE)
```

* Use the str() and summary() functions to get a feel for the dataset.

```{r}
str(crimeData)
summary(crimeData)
```

* Take a look at the data using the "head()" and "tail()" functions

```{r}
head(crimeData)
tail(crimeData)
```

# Data pre-processing

* Check for the number of missing values in the dataset

```{r}
sum(is.na(crimeData))
```


* Convert the State names into row names and remove that variable from the dataset

```{r}
RowNames=crimeData$State
crimeData=data.frame(crimeData,row.names = RowNames)
crimeData=crimeData[,-c(colnames(crimeData) %in% ('State'))]
```

* Standardize and scale the data

```{r}
crime_data_standardised=scale(crimeData,center = T,scale=T)
```

# Data exploration

* Visualise the distances between the individual observations using the fviz_dist()

```{r, fig.width=12, fig.height=8}
library(factoextra)
distance=get_dist(crime_data_standardised)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```

# Hierarchical Clustering

* Cluster the data using the Ward algorithm

```{r}
euclidean_dist=dist(crime_data_standardised,method = 'euclidean')
hc_fit_tree=hclust(euclidean_dist,method = 'ward.D2')
```

* Plot the dendogram for hierarchical clustering

```{r, fig.height=5, fig.width=10}
plot(hc_fit_tree)
```

* Cut the tree to 4 clusters

```{r}
hc_points=cutree(hc_fit_tree,k=4)
crimeDataWith_hc_points=cbind(hc_points,crimeData)
```

* Plot a new dendogram, with each of the clusters being surrounded by a border, using the rect.hclust() function

```{r, fig.height=5, fig.width=10}
plot(hc_fit_tree)
rect.hclust(hc_fit_tree,k=4,border ='blue')
```


# K-Means Clustering

* Build a basic kmeans model with k = 2

```{r}
set.seed(222)
basic_kmeansModel=kmeans(crime_data_standardised,centers = 2)
str(basic_kmeansModel)
```

* Build a scree plot and choose a "k"

```{r}
withinSumOfSq=0
for (i in 1:8) {
  cfit=kmeans(crime_data_standardised,centers = i)
  withinSumOfSq[i]=sum(cfit$withinss)
}
plot(1:8,withinSumOfSq,typ='b')
fviz_nbclust(crime_data_standardised, kmeans, method = "wss")

```

* Choose a k and cluster the data

```{r}
#k is 5
km_clust=kmeans(crime_data_standardised,5,nstart = 10)
km_points=km_clust$cluster
crime_data_standardised_withKMPoints=as.data.frame(cbind(km_points,crimeData))
```

* Visualise the clusters by plotting the data points on the first two principal components
```{r, fig.height=5, fig.width=8}
fviz_cluster(km_clust, crimeData)
```

