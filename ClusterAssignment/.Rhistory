births <- scan("/Users/shwetabhat/Desktop/DataScience/Labs/15-jul_TimeSeries_LabActivity/TimeSeries_NYBirths_wR/nybirths.dat")
birthstimeseries
birthstimeseries <- ts(births,frequency=12,start=c(1946,1))
birthstimeseries
plot.ts(birthstimeseries)
birthstimeseriescomponents <-  decompose(birthstimeseries)
birthstimeseriescomponents <-  decompose(birthstimeseries)
plot(birthstimeseriescomponents)
library(forecast)
library(forecast)
install.packages("forecast")
rm(list = ls(all(TRUE)))
rm(list = ls(all=TRUE))
sales=read.csv("/Users/shwetabhat/Desktop/DataScience/TimeSeries",header = TRUE)
sales=read.csv("/Users/shwetabhat/Desktop/DataScience/TimeSeries/Sales.csv",header = TRUE)
salests=ts(sales,frequency = 4)
salests
rm(list = ls(all=TRUE))
sales=read.csv("/Users/shwetabhat/Desktop/DataScience/TimeSeries/Sales.csv",header = TRUE)
View(sales)
salests=ts(sales,frequency = 4)
salests=ts(sales,frequency = 4)
salests
salests$time=seq(1:20)
sales
class(sales)
sales$time=seq(1:20)
edit(sales)
library(XQuartz)
install.packages("XQuartz")
sales_lm=lm(formula = QuarterSales~.,data = sales)
sales_PloyM=lm(formula = QuarterSales~poly(time,raw = TRUE,degree=2))
sales_PloyM=lm(formula = QuarterSales~poly(time,raw = TRUE,degree=2),data = sales)
points(sales,predict(sales_lm),type = 'l',col='red',lwd=2)
points(sales,predict(sales_PloyM),type = '2',col='red',lwd=2)
points(sales,predict(sales_lm),type = 'l',col='red',lwd=2)
predict(sales_lm)
sales
points(sales$time,predict(sales_lm),type = 'l',col='red',lwd=2)
plot(sales$QuarterSales,type = 'l')
points(sales$time,predict(sales_lm),type = 'l',col='red',lwd=2)
points(sales$time,predict(sales_PloyM),type = '2',col='red',lwd=2)
sales_PloyM=lm(formula = QuarterSales~poly(time,raw = TRUE,degree=2),data = sales)
points(sales$time,predict(sales_lm),type = 'l',col='red',lwd=2)
points(sales$time,predict(sales_PloyM),type = '2',col='blue',lwd=2)
points(sales$time,predict(sales_lm),type = 'l',col='red',lwd=2)
points(sales$time,predict(sales_PloyM),type = 'l',col='blue',lwd=2)
sales$Quarters=seq(c(1:4),rep=5)
sales$Quarters=rep(1:4,5)
sales$Quarters
sales$Quarters=as.factor(sales$Quarters)
sales$Quarters
sales$Quarters=as.factor(as.character(sales$Quarters))
sales$Quarters
sales_lm_seasonal=lm(formula = QuarterSales~ploy(time,degree=2,raw=T)~Quarters,data=sales)
sales_lm_seasonal=lm(formula = QuarterSales~poly(time,raw = TRUE,degree=2)~Quarters,data = sales)
sales_lm_seasonal=lm(formula = sales$QuarterSales~poly(sales$time,raw = TRUE,degree=2)~sales$Quarters,data = sales)
sales_lm_seasonal=lm(formula = QuarterSales~poly(time,raw = TRUE,degree=2),data = sales)
sales_lm_seasonal=lm(formula = QuarterSales~poly(time,raw = TRUE,degree=2)~Quarters,data = sales)
sales$Quarters=rep(1:4,5)
sales$Quarters=as.factor(rep(1:4,5))
sales_lm_seasonal=lm(formula = QuarterSales~poly(time,raw = TRUE,degree=2)~Quarters,data = sales)
,data = s
sales_lm_seasonal=lm(formula = QuarterSales~poly(time,raw = TRUE,degree=2),data = sales)
sales_lm_seasonal=lm(formula = QuarterSales~poly(time,raw = TRUE,degree=2)~Quarters,data = sales)
(sales$Quarters)
sales_lm_seasonal=lm(formula = QuarterSales~poly(time,raw = TRUE,degree=2)+Quarters,data = sales)
points(sales$time,predict(sales_lm_seasonal),type = 'l',col='green')
miles
miles = read.csv("/Users/shwetabhat/Downloads/20170715_CSE_7302c_Batch_30_TimeSeriesLectureNotes/us-air-carrier-traffic-statistic.csv")
miles
miles <- data.frame(miles)
miles$time <- seq(1:204)
edit(miles)
plot(miles$miles, type="l")
lm1 <- lm(miles$miles ~ miles$time)
lm2 <- lm(miles$miles ~
poly(miles$time, 2, raw=TRUE))
lm3 <- lm(miles$miles ~
poly(miles$time, 3, raw=TRUE))
points(miles$time, predict(lm1),
type="l", col="red", lwd=2)
points(miles$time, predict(lm2),
type="l", col="green", lwd=2)
points(miles$time, predict(lm3),
type="l", col="blue", lwd=2)
miles$seasonal <- as.factor(rep(c(1:12),17))
edit(miles)
lm1s <- lm(miles ~ ., data=miles)
lm2s <- lm(miles ~ poly(time, 2, raw=TRUE)+
seasonal, data=miles)
lm3s <- lm(miles ~ poly(time, 3, raw=TRUE)+
seasonal, data=miles)
plot(miles$miles, type="l")
points(miles$time, predict(lm1s),
type="l", col="red", lwd=2)
points(miles$time, predict(lm2s),
type="l", col="blue", lwd=2)
miles$mae <- miles$miles/predict(lm1)
miles$mae <- miles$miles/predict(lm1)
head(miles$mae)
seasonal <- tapply(miles$mae,
miles$seasonal, mean)
seasonal
milespr <- predict(lm1)*rep(seasonal,17)
plot(miles$miles, type="l")
points(miles$time, milespr,
type="l", col="red", lwd=2)
miles$mae <- miles$miles-predict(lm1)
edit(miles)
seasonalAdd <- tapply(miles$mae,
miles$seasonal, mean)
seasonalAdd
milespr <- predict(lm1)+rep(seasonalAdd,17)
plot(miles$miles, type="l")
points(miles$time, milespr,
type="l", col="blue", lwd=2)
library(TTR)
par(mfrow=c(1,1))
milestimeseries
plot(milestimeseries)
smamiles <- SMA(milestimeseries, n=2)
smamiles
wmamiles <- WMA(milestimeseries, n=2)
wmamiles
emamiles <- EMA(milestimeseries, n=2)
emamiles
par(mfrow=c(1,1))
plot(milestimeseries, type="l", col="black")
lines(smamiles, col="red", lwd=2)
lines(wmamiles, col="blue")
lines(emamiles, col="brown")
emamiles <- EMA(milestimeseries, n=2)
milestimeseries <- ts(miles, frequency = 12, start = c(1996,1))
par(mfrow=c(1,1))
milestimeseries
plot(milestimeseries)
smamiles <- SMA(milestimeseries, n=2)
smamiles
wmamiles <- WMA(milestimeseries, n=2)
wmamiles
emamiles <- EMA(milestimeseries, n=2)
emamiles
par(mfrow=c(1,1))
plot(milestimeseries, type="l", col="black")
lines(smamiles, col="red", lwd=2)
lines(wmamiles, col="blue")
lines(emamiles, col="brown")
smamiles <- SMA(milestimeseries, n=2)
library(TTR)
par(mfrow=c(1,1))
milestimeseries
plot(milestimeseries)
smamiles <- SMA(milestimeseries, n=2)
plot(milestimeseries, type="l", col="black")
lines(smamiles, col="red", lwd=2)
smamiles <- SMA(milestimeseries, n=2)
SMA(x = milestimeseries,n=2)
---
title: "Cluster Analysis for Crime Data"
author: "Clustering INSOFE Lab Assignment"
date: "16 July 2017"
output:
html_document:
toc: yes
toc_depth: 3
toc_float: yes
---
**NOTE** Before starting this assignment please remember to clear your environment, you can do that by running the following code chunk
```{r}
rm(list = ls(all=TRUE))
```
# Agenda
* Get the data
* Data pre-processing
* Explore the data
* Hierarchical Clustering
* Kmeans Clustering
* Visualising Clusters and Evaluation
# Problem Description
* In the following Unsupervised Learning activity, you will perform cluster analysis on a dataset that has arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states
* The variable names in the dataset are self explanatory
* So, you will cluster US states based on the crime rates, which can then be passed on to public policy experts to gain insight from it
# Reading & Understanding the Data
* Read in the dataset
```{r}
# Use the setwd() function to get to the directory where the data is present
setwd("/Users/shwetabhat/Desktop/DataScience/ClusterAssignment")
crimeData=read.csv("/Users/shwetabhat/Desktop/DataScience/ClusterAssignment/crime_data.csv",header=TRUE)
```
* Use the str() and summary() functions to get a feel for the dataset.
```{r}
str(crimeData)
summary(crimeData)
```
* Take a look at the data using the "head()" and "tail()" functions
```{r}
head(crimeData)
tail(crimeData)
```
# Data pre-processing
* Check for the number of missing values in the dataset
```{r}
sum(is.na(crimeData))
```
* Convert the State names into row names and remove that variable from the dataset
```{r}
RowNames=crimeData$State
crimeData=data.frame(crimeData,row.names = RowNames)
crimeData=crimeData[,-c(colnames(crimeData) %in% ('State'))]
```
* Standardize and scale the data
```{r}
crime_data_standardised=scale(crimeData,center = T,scale=T)
```
# Data exploration
* Visualise the distances between the individual observations using the fviz_dist()
```{r, fig.width=12, fig.height=8}
library(factoextra)
distance=get_dist(crime_data_standardised)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```
# Hierarchical Clustering
* Cluster the data using the Ward algorithm
```{r}
dist=dist(crime_data_standardised,method = 'euclidean')
hc_fit_points=hclust(crime_data_standardised,method = 'ward.D2')
crimeData=cbind(hc_fit_points,crimeData)
hc_fit_points=hclust(crime_data_standardised,method = 'ward.D2')
dim(hc_fit_points)
dist=dist(crime_data_standardised,method = 'euclidean')
hc_fit_points=hclust(crime_data_standardised,method = 'ward.D2')
sum(is.na(crime_data_standardised))
dist=dist(crime_data_standardised,method = 'euclidean')
hc_fit_points=hclust(crime_data_standardised,method = 'ward.D2')
dim(crime_data_standardised)
dist
distanc=dist(crime_data_standardised,method = 'euclidean')
hc_fit_points=hclust(crime_data_standardised,method = 'ward.D2')
crime_data_standardised
sum(is.na(crimeData))
hc_fit_points=hclust(distanc,method = 'ward.D2')
crimeData=cbind(hc_fit_points,crimeData)
str(hc_fit_points)
crimeData=cbind(data.frame(hc_fit_points),crimeData)
plot(hc_fit_points)
hc_points=cutree(hc_fit_tree,k=2)
hc_fit_tree=hclust(distanc,method = 'ward.D2')
hc_points=cutree(hc_fit_tree,k=2)
crimeData=cbind(data.frame(hc_fit_points),crimeData)
hc_points=cutree(hc_fit_tree,k=2)
str(hc_fit_points)
crimeData=cbind(hc_fit_points,crimeData)
crimeData=cbind(hc_points,crimeData)
plot(crimeData)
plot(hc_fit_tree)
rect.hclust(hc_fit_tree,k=2,col='blue')
rect.hclust(hc_fit_tree,k=2,border ='blue')
cereals_data = read.csv('/Users/shwetabhat/Desktop/DataScience/Labs/16-july_ClusterAnalysis_ClassLabActivity/Cereals.csv', header = T)
library(DMwR)
cereals_data <- knnImputation(cereals_data, k = 3, scale = T)
rownames(cereals_data) <- cereals_data$name
cereals_data <- cereals_data[, -c(colnames(cereals_data) %in% ("name"))]
# (or)
cereals_data$name = NULL
cereals_data <- scale(cereals_data, center = T, scale = T)
cereals_data
data = read.csv("/Users/shwetabhat/Desktop/DataScience/Labs/20170730_Batch30_CSE7305c_KNN_CBF_LabActivity/MovieRatings.csv",header=T)
head(data,30)
tail(data,30)
numOfUsers <-length(unique(data$UserID))
print(numOfUsers)
rm(list=ls(all=T))
# Load the required libraries at one place as a good practice
library(lsa)
library(reshape2)
library(vegan)
data = read.csv("/Users/shwetabhat/Desktop/DataScience/Labs/20170730_Batch30_CSE7305c_KNN_CBF_LabActivity/MovieRatings.csv",header=T)
head(data,30)
numOfUsers <-length(unique(data$UserID))
print(numOfUsers)
data2=dcast(data, UserID ~ Movie, value="Rating")
data2=data2[,c("UserID","M1","M2","M3","M4","M5","M6","M7","M8","M9","M10","M11","M12","M13","M14","M15","M16","M17","M18","M19")]
data2
data2 <-data2[1:100,]
data2
data2=dcast(data, UserID ~ Movie, value="Rating")
data2=data2[,c("UserID","M1","M2","M3","M4","M5","M6","M7","M8","M9","M10","M11","M12","M13","M14","M15","M16","M17","M18","M19")]
data2
?dcast
rm(list=ls(all=T))
library(lsa)
library(reshape2)
library(vegan)
data = read.csv("/Users/shwetabhat/Desktop/DataScience/Labs/20170730_Batch30_CSE7305c_KNN_CBF_LabActivity/MovieRatings.csv",header=T)
head(data,30)
tail(data,30)
data
str(data)
data2=dcast(data, UserID ~ Movie, value="Rating")
data2
str(data2)
head(data)
head(data,25)
str(data)
data1 <- data2[,-1]
str(data1)
str(data1)
str(data)
str(data2)
dist = 0
user = 0
