births <- scan("/Users/shwetabhat/Desktop/DataScience/Labs/15-jul_TimeSeries_LabActivity/TimeSeries_NYBirths_wR/nybirths.dat")
birthstimeseries
birthstimeseries <- ts(births,frequency=12,start=c(1946,1))
birthstimeseries
plot.ts(birthstimeseries)
birthstimeseriescomponents <-  decompose(birthstimeseries)
birthstimeseriescomponents <-  decompose(birthstimeseries)
plot(birthstimeseriescomponents)
library(forecast)
library(forecast)
install.packages("forecast")
rm(list = ls(all(TRUE)))
rm(list = ls(all=TRUE))
sales=read.csv("/Users/shwetabhat/Desktop/DataScience/TimeSeries",header = TRUE)
sales=read.csv("/Users/shwetabhat/Desktop/DataScience/TimeSeries/Sales.csv",header = TRUE)
salests=ts(sales,frequency = 4)
salests
rm(list = ls(all=TRUE))
sales=read.csv("/Users/shwetabhat/Desktop/DataScience/TimeSeries/Sales.csv",header = TRUE)
View(sales)
salests=ts(sales,frequency = 4)
salests=ts(sales,frequency = 4)
salests
salests$time=seq(1:20)
sales
class(sales)
sales$time=seq(1:20)
edit(sales)
library(XQuartz)
install.packages("XQuartz")
sales_lm=lm(formula = QuarterSales~.,data = sales)
sales_PloyM=lm(formula = QuarterSales~poly(time,raw = TRUE,degree=2))
sales_PloyM=lm(formula = QuarterSales~poly(time,raw = TRUE,degree=2),data = sales)
points(sales,predict(sales_lm),type = 'l',col='red',lwd=2)
points(sales,predict(sales_PloyM),type = '2',col='red',lwd=2)
points(sales,predict(sales_lm),type = 'l',col='red',lwd=2)
predict(sales_lm)
sales
points(sales$time,predict(sales_lm),type = 'l',col='red',lwd=2)
plot(sales$QuarterSales,type = 'l')
points(sales$time,predict(sales_lm),type = 'l',col='red',lwd=2)
points(sales$time,predict(sales_PloyM),type = '2',col='red',lwd=2)
sales_PloyM=lm(formula = QuarterSales~poly(time,raw = TRUE,degree=2),data = sales)
points(sales$time,predict(sales_lm),type = 'l',col='red',lwd=2)
points(sales$time,predict(sales_PloyM),type = '2',col='blue',lwd=2)
points(sales$time,predict(sales_lm),type = 'l',col='red',lwd=2)
points(sales$time,predict(sales_PloyM),type = 'l',col='blue',lwd=2)
sales$Quarters=seq(c(1:4),rep=5)
sales$Quarters=rep(1:4,5)
sales$Quarters
sales$Quarters=as.factor(sales$Quarters)
sales$Quarters
sales$Quarters=as.factor(as.character(sales$Quarters))
sales$Quarters
sales_lm_seasonal=lm(formula = QuarterSales~ploy(time,degree=2,raw=T)~Quarters,data=sales)
sales_lm_seasonal=lm(formula = QuarterSales~poly(time,raw = TRUE,degree=2)~Quarters,data = sales)
sales_lm_seasonal=lm(formula = sales$QuarterSales~poly(sales$time,raw = TRUE,degree=2)~sales$Quarters,data = sales)
sales_lm_seasonal=lm(formula = QuarterSales~poly(time,raw = TRUE,degree=2),data = sales)
sales_lm_seasonal=lm(formula = QuarterSales~poly(time,raw = TRUE,degree=2)~Quarters,data = sales)
sales$Quarters=rep(1:4,5)
sales$Quarters=as.factor(rep(1:4,5))
sales_lm_seasonal=lm(formula = QuarterSales~poly(time,raw = TRUE,degree=2)~Quarters,data = sales)
,data = s
sales_lm_seasonal=lm(formula = QuarterSales~poly(time,raw = TRUE,degree=2),data = sales)
sales_lm_seasonal=lm(formula = QuarterSales~poly(time,raw = TRUE,degree=2)~Quarters,data = sales)
(sales$Quarters)
sales_lm_seasonal=lm(formula = QuarterSales~poly(time,raw = TRUE,degree=2)+Quarters,data = sales)
points(sales$time,predict(sales_lm_seasonal),type = 'l',col='green')
miles
miles = read.csv("/Users/shwetabhat/Downloads/20170715_CSE_7302c_Batch_30_TimeSeriesLectureNotes/us-air-carrier-traffic-statistic.csv")
miles
miles <- data.frame(miles)
miles$time <- seq(1:204)
edit(miles)
plot(miles$miles, type="l")
lm1 <- lm(miles$miles ~ miles$time)
lm2 <- lm(miles$miles ~
poly(miles$time, 2, raw=TRUE))
lm3 <- lm(miles$miles ~
poly(miles$time, 3, raw=TRUE))
points(miles$time, predict(lm1),
type="l", col="red", lwd=2)
points(miles$time, predict(lm2),
type="l", col="green", lwd=2)
points(miles$time, predict(lm3),
type="l", col="blue", lwd=2)
miles$seasonal <- as.factor(rep(c(1:12),17))
edit(miles)
lm1s <- lm(miles ~ ., data=miles)
lm2s <- lm(miles ~ poly(time, 2, raw=TRUE)+
seasonal, data=miles)
lm3s <- lm(miles ~ poly(time, 3, raw=TRUE)+
seasonal, data=miles)
plot(miles$miles, type="l")
points(miles$time, predict(lm1s),
type="l", col="red", lwd=2)
points(miles$time, predict(lm2s),
type="l", col="blue", lwd=2)
miles$mae <- miles$miles/predict(lm1)
miles$mae <- miles$miles/predict(lm1)
head(miles$mae)
seasonal <- tapply(miles$mae,
miles$seasonal, mean)
seasonal
milespr <- predict(lm1)*rep(seasonal,17)
plot(miles$miles, type="l")
points(miles$time, milespr,
type="l", col="red", lwd=2)
miles$mae <- miles$miles-predict(lm1)
edit(miles)
seasonalAdd <- tapply(miles$mae,
miles$seasonal, mean)
seasonalAdd
milespr <- predict(lm1)+rep(seasonalAdd,17)
plot(miles$miles, type="l")
points(miles$time, milespr,
type="l", col="blue", lwd=2)
library(TTR)
par(mfrow=c(1,1))
milestimeseries
plot(milestimeseries)
smamiles <- SMA(milestimeseries, n=2)
smamiles
wmamiles <- WMA(milestimeseries, n=2)
wmamiles
emamiles <- EMA(milestimeseries, n=2)
emamiles
par(mfrow=c(1,1))
plot(milestimeseries, type="l", col="black")
lines(smamiles, col="red", lwd=2)
lines(wmamiles, col="blue")
lines(emamiles, col="brown")
emamiles <- EMA(milestimeseries, n=2)
milestimeseries <- ts(miles, frequency = 12, start = c(1996,1))
par(mfrow=c(1,1))
milestimeseries
plot(milestimeseries)
smamiles <- SMA(milestimeseries, n=2)
smamiles
wmamiles <- WMA(milestimeseries, n=2)
wmamiles
emamiles <- EMA(milestimeseries, n=2)
emamiles
par(mfrow=c(1,1))
plot(milestimeseries, type="l", col="black")
lines(smamiles, col="red", lwd=2)
lines(wmamiles, col="blue")
lines(emamiles, col="brown")
smamiles <- SMA(milestimeseries, n=2)
library(TTR)
par(mfrow=c(1,1))
milestimeseries
plot(milestimeseries)
smamiles <- SMA(milestimeseries, n=2)
plot(milestimeseries, type="l", col="black")
lines(smamiles, col="red", lwd=2)
smamiles <- SMA(milestimeseries, n=2)
SMA(x = milestimeseries,n=2)
uninsta
remove.packages(rattle)
remove.packages(GTk2)
remove.packages(RGTk2)
install.packages("rpart")
reg_tree <- rpart(imdb_score ~ ., train_reg)
tit_data=read.csv(""/Users/shwetabhat/Desktop/DataScience/Labs/20170723_Batch30_CSE7305_DecisionTrees_activity/titanic_data.csv",header=TRUE)
tit_data=read.csv("/Users/shwetabhat/Desktop/DataScience/Labs/20170723_Batch30_CSE7305_DecisionTrees_activity/titanic_data.csv",header=TRUE)
str(tit_data)
summary(tit_data)
sum(is.na(tit_data))
train_rows=sample(1:nrows(tit_data)),.7*nrows(tit_data))
train_rows=sample(1:nrows(tit_data),0.7*nrows(tit_data))
str(tit_data)
summary(tit_data)
sum(is.na(tit_data))
set.seed(100)
train_rows=sample(1:nrows(tit_data),0.7*nrows(tit_data))
reg_tree1=rpart(Survived~.,data=tit_data)
library(rpart)
library(rpart.plot)
install.packages(rpart.plot)
install.packages(rpart.plot)
install.packages("rpart")
install.packages("rpart.plot")
library(rpart.plot)
library(rpart)
rpart.plot(reg_tree1)
reg_tree1=rpart(Survived~.,data=tit_data)
rpart.plot(reg_tree1)
reg_tree1$variable.importance
printcp(reg_tree1)
asRules(reg_tree1)
pred_reg1=predict(reg_tree1, tit_data)
library(DMwR)
regr.eval(test_reg$imdb_score, preds_reg)
pred_reg1=predict(tit_data$Survived, reg_tree1)
library(DMwR)
pred_reg1=predict(tit_data$Survived, reg_tree1)
pred_reg1=predict(reg_tree1,tit_data)
library(DMwR)
library(DMwR)
regr.eval(tit_data$Survived,pred_reg1)
pred_reg1
regr.eval(tit_data$Survived,pred_reg1)
reg_tree <- rpart(imdb_score ~ ., train_reg)
mov_data <- read.csv("/Users/shwetabhat/Desktop/DataScience/Labs/20170723_Batch30_CSE7305_DecisionTrees_activity/movie_data.csv", na.strings = "")
c("color", "num_critic_for_reviews", "duration", "director_facebook_likes",
"gross", "cast_total_facebook_likes", "num_user_for_reviews", "budget",
"movie_facebook_likes", "imdb_score")]
set.seed(1234)
train_rows <- sample(1:nrow(movie_data), 0.7*nrow(movie_data))
train_reg <- movie_data[train_rows, ]
test_reg <- movie_data[-train_rows, ]
library(DMwR)
train_reg <- knnImputation(train_reg, k = 5, scale = T)
set.seed(1234)
train_rows <- sample(1:nrow(movie_data), 0.7*nrow(movie_data))
train_reg <- movie_data[train_rows, ]
test_reg <- movie_data[-train_rows, ]
set.seed(1234)
train_rows <- sample(1:nrow(movie_data), 0.7*nrow(movie_data))
movie_data <- mov_data[, names(mov_data) %in%
c("color", "num_critic_for_reviews", "duration", "director_facebook_likes",
"gross", "cast_total_facebook_likes", "num_user_for_reviews", "budget",
"movie_facebook_likes", "imdb_score")]
set.seed(1234)
train_rows <- sample(1:nrow(movie_data), 0.7*nrow(movie_data))
train_reg <- movie_data[train_rows, ]
test_reg <- movie_data[-train_rows, ]
library(DMwR)
train_reg <- knnImputation(train_reg, k = 5, scale = T)
test_reg <- knnImputation(test_reg, 5, scale = T, distData = train_reg)
library(rpart)
reg_tree <- rpart(imdb_score ~ ., train_reg)
printcp(reg_tree)
reg_tree$variable.importance
library(rpart.plot)
rpart.plot(reg_tree)
preds_reg <- predict(reg_tree, test_reg)
preds_reg
library(DMwR)
regr.eval(test_reg$imdb_score, preds_reg)
tit_data=read.csv("/Users/shwetabhat/Desktop/DataScience/Labs/20170723_Batch30_CSE7305_DecisionTrees_activity/titanic_data.csv",header=TRUE)
str(tit_data)
summary(tit_data)
sum(is.na(tit_data))
reg_tree1=rpart(Survived~.,data=tit_data)
rpart.plot(reg_tree1)
reg_tree1$variable.importance
printcp(reg_tree1)
pred_reg1=predict(reg_tree1,tit_data)
library(DMwR)
regr.eval(tit_data$Survived,pred_reg1)
pred_reg1=predict(reg_tree1,tit_data)
??rpart
reg_tree1=rpart(Survived~.,data=tit_data,model ="class" )
rpart.plot(reg_tree1)
reg_tree1$variable.importance
printcp(reg_tree1)
pred_reg1=predict(reg_tree1,tit_data)
library(DMwR)
regr.eval(tit_data$Survived,pred_reg1)
summary(pred_reg1)
table(pred_reg1)
---
title: "Prediction of Patient Status using Voice Measurements"
author: "Shweta Bhat."
date: "8 July 2017"
output:
html_document:
toc: yes
toc_depth: 3
toc_float: yes
---
**NOTE** Before starting this assignment please remember to clear your environment, you can do that by running the following code chunk
```{r}
rm(list = ls(all=TRUE))
```
# Agenda
* Get the data
* Data Pre-processing
* Build a model
* Predictions
* Communication
# Reading & Understanding the Data
* The "parkinsons_data.csv"" dataset is composed of a range of biomedical voice measurements. Each column in the table is a particular voice measure, and each row corresponds one of 195 voice recordings.
* The dataset has 195 rows and 23 columns.
* The column/variable names' explanation is given below:
1) __MDVP:Fo(Hz) :__ Average vocal fundamental frequency
2) __MDVP:Fhi(Hz) :__ Maximum vocal fundamental frequency
3) __MDVP:Flo(Hz) :__ Minimum vocal fundamental frequency
4) __MDVP:Jitter(%); MDVP:Jitter(Abs); MDVP:RAP; MDVP:PPQ; Jitter:DDP :___ Several measures of variation in fundamental frequency
5) __MDVP:Shimmer; MDVP:Shimmer(dB); Shimmer:APQ3; Shimmer:APQ5; MDVP:APQ; Shimmer:DDA :__ Several measures of variation in amplitude
6) __NHR; HNR :__ Two measures of ratio of noise to tonal components in the voice
7) __status :__ Health status of the subject (Parkinson's) - Positive, (Normal) - Healthy
8) __RPDE; D2 :__  Two nonlinear dynamical complexity measures
9) __DFA :__ Signal fractal scaling exponent
10) __spread1; spread2; PPE :__ Three nonlinear measures of fundamental frequency variation
* Make sure the dataset is located in your current working directory and read in the data
```{r}
setwd("/Users/shwetabhat/Desktop/DataScience/LogisticRegression_Assignment")
parkisons_data=read.csv("/Users/shwetabhat/Desktop/DataScience/LogisticRegression_Assignment/parkinsons_data.csv",header=TRUE)
```
* Use the str() function to get a feel for the dataset.
```{r}
str(parkisons_data)
```
* Take a look at the data using the "head()" and "tail()" functions
```{r}
head(parkisons_data)
tail(parkisons_data)
```
* Are there any missing values in the dataset?
```{r}
sum(is.na(parkisons_data))
```
# Data Pre-processing
## Train/Test Split
* Split the data 70/30 into train and test sets, using __Stratified Sampling__ by setting the seed as "786"
```{r}
library(caret)
set.seed(222)
summary(parkisons_data$status)
#has unequal classes.Number of Normal is different to Parkinsons
trained_rows <- createDataPartition(parkisons_data$status, p = 0.7, list = F)
##sample(1:nrow(parkisons_data), 0.7*nrow(parkisons_data))
trained_data <- parkisons_data[trained_rows, ]
testing_data <- parkisons_data[-trained_rows, ]
```
# Build a model
## Basic Logistic Regression Model
* Use the glm() function to build a basic model
* Build a model using all the variables, excluding the response variable, in the dataset
```{r}
logistic_model=glm(formula=status~.,data = trained_data,family='binomial')
```
* Get the summary of the model and understand the output
```{r}
summary(logistic_model)
#Only 1 variable MDVP.Fo.Hz. seems to be significant at 5% significance level.
library(MASS)
stepAICModel=stepAIC(logistic_model)
#StepAIC gives formula MDVP.Fo.Hz. + MDVP.Jitter... + MDVP.Jitter.Abs. + MDVP.RAP + RPDE + spread2 + PPE
#indicating variables MDVP.Fo.Hz., MDVP.Jitter,MDVP.Jitter.Abs.,MDVP.RAP, RPDE,spread2,PPE are important
library(car)
logistic_stepAIC_vif = vif(stepAICModel)
logistic_stepAIC_vif
#Says attributes RPDE,spread2,PPE  all have VIF less than 5 i.e, multi colinearity problem.so have to remove some attriibutes in model builiding.
#rebuilding model.
logistic_model1 = glm(formula=status ~ MDVP.Fo.Hz.+MDVP.Jitter... + MDVP.Jitter.Abs.+MDVP.RAP+RPDE+PPE+spread2,data = trained_data,family='binomial')
summary(logistic_model1)
```
## Create an ROC Curve
1) Get a list of predictions (probability scores) using the predict() function
```{r}
library(ROCR)
prob_Resp=predict(logistic_model1,type = "response")
```
2) Using the ROCR package create a "prediction()" object
```{r}
predicted_resp=prediction(prob_Resp,trained_data$status)
```
3) Extract performance measures (True Positive Rate and False Positive Rate) using the "performance()" function from the ROCR package
```{r}
perf = performance(predicted_resp, measure="tpr", x.measure="fpr")
```
4) Plot the ROC curve using the extracted performance measures (TPR and FPR)
```{r}
plot(perf, col=rainbow(10), colorize=T, print.cutoffs.at=seq(0,1,.05))
```
* Extract the AUC score of the ROC curve and store it in a variable named "auc"
```{r}
performance_auc <- performance(predicted_resp, measure="auc")
# Access the auc score from the performance object
auc <- performance_auc@y.values[[1]]
print(auc)
```
## Choose a Cutoff Value
* Based on the trade off between TPR and FPR depending on the business domain, a call on the cutoff has to be made.
```{r}
### Write your answer here
# A cutoff of ----- seems reasonable
#.1 or .05 seems to be reasonable for my assuption.
```
## Predictions on test data
* After choosing a cutoff value, predict the class labels on the test data using our model
```{r}
prob_test_Data=predict(logistic_model1, testing_data, type = "response")
preds_test=ifelse(prob_test_Data > 0.1, "Parkinson's", "Normal")
table(preds_test)
```
# Evaluation Metrics for classification
## Manual Computation
### Confusion Matrix
* Create a confusion matrix using the table() function
```{r}
Confusion_matrix=table(testing_data$status, preds_test)
print(Confusion_matrix)
